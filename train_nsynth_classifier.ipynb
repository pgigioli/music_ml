{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\n",
      "  from numba.decorators import jit as optional_jit\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import datetime\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "\n",
    "from src.data import NSynth\n",
    "from src.utils import print_and_log\n",
    "from src.models import Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():  \n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:  \n",
    "    device = torch.device(\"cpu\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    'checkpoint' : None,\n",
    "    'instrument_source' : [0, 1, 2],\n",
    "    'sample_rate' : 16000,\n",
    "    'n_samples' : 64000,\n",
    "    'feature_type' : 'mel',\n",
    "    'random_crop' : True,\n",
    "    'resize' : None,\n",
    "    'normalize' : True,\n",
    "    'standardize' : True,\n",
    "    'standardize_mean' : 0.3356,\n",
    "    'standardize_std' : 0.2212,\n",
    "    'spec_augment' : True,\n",
    "    'remove_synth_lead' : True,\n",
    "    'n_samples_per_class' : None,\n",
    "    'n_epochs' : 50,\n",
    "    'batch_size' : 32,\n",
    "    'lr' : 0.000001,\n",
    "    'hidden_dim' : 512,\n",
    "    'display_iters' : 100,\n",
    "    'val_iters' : 1000,\n",
    "    'n_val_samples' : 1000, \n",
    "    'n_early_stopping' : 5 # stop if validation doesn't improve after this number of validation cycles\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NSynth(\n",
    "    'data/nsynth/nsynth-train', \n",
    "    include_meta=True, \n",
    "    instrument_source=hparams['instrument_source'], \n",
    "    sample_rate=hparams['sample_rate'], \n",
    "    n_samples=hparams['n_samples'], \n",
    "    feature_type=hparams['feature_type'], \n",
    "    random_crop=hparams['random_crop'], \n",
    "    resize=hparams['resize'], \n",
    "    normalize=hparams['normalize'], \n",
    "    standardize=hparams['standardize'], \n",
    "    standardize_mean=hparams['standardize_mean'], \n",
    "    standardize_std=hparams['standardize_std'], \n",
    "    spec_augment=hparams['spec_augment'],\n",
    "    remove_synth_lead=hparams['remove_synth_lead'], \n",
    "    n_samples_per_class=hparams['n_samples_per_class']\n",
    ")\n",
    "\n",
    "val_dataset = NSynth(\n",
    "    'data/nsynth/nsynth-valid', \n",
    "    include_meta=True, \n",
    "    instrument_source=hparams['instrument_source'], \n",
    "    sample_rate=hparams['sample_rate'], \n",
    "    n_samples=hparams['n_samples'], \n",
    "    feature_type=hparams['feature_type'], \n",
    "    random_crop=hparams['random_crop'], \n",
    "    resize=hparams['resize'], \n",
    "    normalize=hparams['normalize'], \n",
    "    standardize=hparams['standardize'], \n",
    "    standardize_mean=hparams['standardize_mean'], \n",
    "    standardize_std=hparams['standardize_std'], \n",
    "    remove_synth_lead=hparams['remove_synth_lead']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_ctr = Counter([x['instrument_family_str'] for x in train_dataset.meta.values()])\n",
    "class_dict = dict(enumerate(sorted(class_ctr)))\n",
    "inv_class_dict = dict([(v, k) for k, v in class_dict.items()])\n",
    "class_weights = np.array([max(class_ctr.values())/class_ctr[class_dict[i]] for i in range(len(class_dict))])\n",
    "\n",
    "sample_weights = [max(class_ctr.values())/class_ctr[train_dataset.meta[f]['instrument_family_str']] for f in train_dataset.files]\n",
    "sampler = torch.utils.data.sampler.WeightedRandomSampler(sample_weights, len(sample_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'guitar': 32690,\n",
       "         'bass': 65474,\n",
       "         'organ': 34477,\n",
       "         'keyboard': 51821,\n",
       "         'vocal': 10208,\n",
       "         'string': 19474,\n",
       "         'reed': 13911,\n",
       "         'flute': 8773,\n",
       "         'mallet': 34201,\n",
       "         'brass': 12675})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_ctr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=hparams['batch_size'], sampler=sampler)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=hparams['batch_size'], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of parameters : 2332554\n"
     ]
    }
   ],
   "source": [
    "model = Classifier(n_classes=len(class_dict), h_dim=hparams['hidden_dim']).to(device)\n",
    "print('# of parameters : {}'.format(sum(p.numel() for p in model.parameters() if p.requires_grad)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=hparams['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier\n"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.datetime.fromtimestamp(time.time()).strftime('%Y%m%d-%H%M%S')\n",
    "results_dir = 'train_results/classifier/{}'.format(timestamp)\n",
    "os.makedirs(results_dir)\n",
    "\n",
    "with open(os.path.join(results_dir, 'hparams.json'), 'w') as fp:\n",
    "    json.dump(hparams, fp)\n",
    "    \n",
    "with open(os.path.join(results_dir, 'label_dict.json'), 'w') as fp:\n",
    "    json.dump(class_dict, fp)\n",
    "\n",
    "log_file = os.path.join(results_dir, 'train_log.txt')\n",
    "log = open(log_file, 'w')\n",
    "log.close()\n",
    "print_and_log('{} {}'.format(train_dataset.__class__.__name__, model.__class__.__name__), log_file)\n",
    "\n",
    "for k, v in hparams.items(): print_and_log('{} : {}'.format(k, v), log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0,     1] loss : 2.3782, acc : 0.1250\n",
      "[0,   100] loss : 2.1843, acc : 0.2188\n",
      "[0,   200] loss : 2.2320, acc : 0.1875\n",
      "[0,   300] loss : 2.1437, acc : 0.1250\n",
      "[0,   400] loss : 1.9996, acc : 0.3750\n",
      "[0,   500] loss : 2.0427, acc : 0.2188\n",
      "[0,   600] loss : 2.0638, acc : 0.3125\n",
      "[0,   700] loss : 2.0168, acc : 0.3125\n",
      "[0,   800] loss : 1.7990, acc : 0.4688\n",
      "[0,   900] loss : 1.9511, acc : 0.3438\n",
      "[0,  1000] loss : 1.7725, acc : 0.3750\n",
      "[0,  1100] loss : 1.8843, acc : 0.3750\n",
      "[0,  1200] loss : 1.8353, acc : 0.4062\n",
      "[0,  1300] loss : 1.9304, acc : 0.3438\n",
      "[0,  1400] loss : 1.7487, acc : 0.3750\n",
      "[0,  1500] loss : 1.8844, acc : 0.3750\n",
      "[0,  1600] loss : 1.8353, acc : 0.3750\n",
      "[0,  1700] loss : 1.6468, acc : 0.3750\n",
      "[0,  1800] loss : 1.7506, acc : 0.2500\n",
      "[0,  1900] loss : 1.7861, acc : 0.4375\n",
      "[0,  2000] loss : 1.6724, acc : 0.3750\n",
      "[0,  2100] loss : 1.6526, acc : 0.5000\n",
      "[0,  2200] loss : 1.6899, acc : 0.4062\n",
      "[0,  2300] loss : 1.6156, acc : 0.4062\n",
      "[0,  2400] loss : 1.9391, acc : 0.3125\n",
      "[0,  2500] loss : 1.5113, acc : 0.5938\n",
      "[0,  2600] loss : 1.5465, acc : 0.4375\n",
      "[0,  2700] loss : 1.5709, acc : 0.4688\n",
      "[0,  2800] loss : 1.8554, acc : 0.3438\n",
      "[0,  2900] loss : 1.4149, acc : 0.5000\n",
      "[0,  3000] loss : 1.6103, acc : 0.3750\n",
      "[0,  3100] loss : 1.6467, acc : 0.4062\n",
      "[0,  3200] loss : 1.5717, acc : 0.5625\n",
      "[0,  3300] loss : 1.4150, acc : 0.6250\n",
      "[0,  3400] loss : 1.5740, acc : 0.5000\n",
      "[0,  3500] loss : 1.4979, acc : 0.4688\n",
      "[0,  3600] loss : 1.5782, acc : 0.4688\n",
      "[0,  3700] loss : 1.6672, acc : 0.4688\n",
      "[0,  3800] loss : 1.5390, acc : 0.4062\n",
      "[0,  3900] loss : 1.6177, acc : 0.4375\n",
      "[0,  4000] loss : 1.4812, acc : 0.5312\n",
      "[0,  4100] loss : 1.7158, acc : 0.3438\n",
      "[0,  4200] loss : 1.3800, acc : 0.6250\n",
      "[0,  4300] loss : 1.6160, acc : 0.4062\n",
      "[0,  4400] loss : 1.3614, acc : 0.5000\n",
      "[0,  4500] loss : 1.4342, acc : 0.5625\n",
      "[0,  4600] loss : 1.4915, acc : 0.5000\n",
      "[0,  4700] loss : 1.4394, acc : 0.5000\n",
      "[0,  4800] loss : 1.3892, acc : 0.5938\n",
      "[0,  4900] loss : 1.5308, acc : 0.4375\n",
      "[0,  5000] loss : 1.4570, acc : 0.5312\n",
      "[0,  5100] loss : 1.3215, acc : 0.5312\n",
      "[0,  5200] loss : 1.0886, acc : 0.5938\n",
      "[0,  5300] loss : 1.3740, acc : 0.5938\n",
      "[0,  5400] loss : 1.6958, acc : 0.5000\n",
      "[0,  5500] loss : 1.1933, acc : 0.5938\n",
      "[0,  5600] loss : 1.5254, acc : 0.5000\n",
      "[0,  5700] loss : 1.6059, acc : 0.3750\n",
      "[0,  5800] loss : 1.6029, acc : 0.5312\n",
      "[0,  5900] loss : 1.4466, acc : 0.5000\n",
      "[0,  6000] loss : 1.4413, acc : 0.5000\n",
      "[0,  6100] loss : 1.2178, acc : 0.6250\n",
      "[0,  6200] loss : 1.2222, acc : 0.6250\n",
      "[0,  6300] loss : 1.2827, acc : 0.5625\n",
      "[0,  6400] loss : 1.2655, acc : 0.5625\n",
      "[0,  6500] loss : 1.5461, acc : 0.4375\n",
      "[0,  6600] loss : 1.2892, acc : 0.5625\n",
      "[0,  6700] loss : 1.1799, acc : 0.6562\n",
      "[0,  6800] loss : 1.2604, acc : 0.6562\n",
      "[0,  6900] loss : 1.2959, acc : 0.4688\n",
      "[0,  7000] loss : 1.1335, acc : 0.5938\n",
      "[0,  7100] loss : 1.0744, acc : 0.5938\n",
      "[0,  7200] loss : 1.4477, acc : 0.5000\n",
      "[0,  7300] loss : 1.4078, acc : 0.5312\n",
      "[0,  7400] loss : 1.0730, acc : 0.7188\n",
      "[0,  7500] loss : 1.2013, acc : 0.5625\n",
      "[0,  7600] loss : 1.0760, acc : 0.7188\n",
      "[0,  7700] loss : 1.0803, acc : 0.6250\n",
      "[0,  7800] loss : 1.0020, acc : 0.7188\n",
      "[0,  7900] loss : 1.2510, acc : 0.6250\n",
      "[0,  8000] loss : 1.2614, acc : 0.5625\n",
      "[0,  8100] loss : 1.0912, acc : 0.6250\n",
      "[0,  8200] loss : 1.1701, acc : 0.6562\n",
      "[0,  8300] loss : 1.1913, acc : 0.6250\n",
      "[0,  8400] loss : 1.0255, acc : 0.6250\n",
      "[0,  8500] loss : 1.0972, acc : 0.6562\n",
      "[0,  8600] loss : 1.3998, acc : 0.5938\n",
      "[0,  8700] loss : 1.3907, acc : 0.5938\n",
      "[0,  8800] loss : 1.0318, acc : 0.6875\n",
      "Val - loss : 1.2922, acc : 0.5508\n",
      "Weights saved in train_results/classifier/20200723-182433/model-8866.weights\n",
      "[1,  8900] loss : 1.1217, acc : 0.6875\n",
      "[1,  9000] loss : 1.1884, acc : 0.6875\n",
      "[1,  9100] loss : 0.8955, acc : 0.7500\n",
      "[1,  9200] loss : 1.1173, acc : 0.6250\n",
      "[1,  9300] loss : 1.4274, acc : 0.5312\n",
      "[1,  9400] loss : 1.2296, acc : 0.7500\n",
      "[1,  9500] loss : 1.2650, acc : 0.5625\n",
      "[1,  9600] loss : 1.0263, acc : 0.7188\n",
      "[1,  9700] loss : 1.1177, acc : 0.5938\n",
      "[1,  9800] loss : 1.3772, acc : 0.4688\n",
      "[1,  9900] loss : 1.4613, acc : 0.5312\n",
      "[1, 10000] loss : 1.2186, acc : 0.5312\n",
      "[1, 10100] loss : 1.1183, acc : 0.6250\n",
      "[1, 10200] loss : 1.1140, acc : 0.5938\n",
      "[1, 10300] loss : 1.0517, acc : 0.7188\n",
      "[1, 10400] loss : 1.1674, acc : 0.6562\n",
      "[1, 10500] loss : 1.2320, acc : 0.6875\n",
      "[1, 10600] loss : 0.9758, acc : 0.7188\n",
      "[1, 10700] loss : 1.1616, acc : 0.5312\n",
      "[1, 10800] loss : 1.4550, acc : 0.4688\n",
      "[1, 10900] loss : 0.7278, acc : 0.8125\n",
      "[1, 11000] loss : 0.8354, acc : 0.7188\n",
      "[1, 11100] loss : 0.8327, acc : 0.7188\n",
      "[1, 11200] loss : 0.9470, acc : 0.6562\n",
      "[1, 11300] loss : 0.9615, acc : 0.7188\n",
      "[1, 11400] loss : 0.7253, acc : 0.7812\n",
      "[1, 11500] loss : 1.0033, acc : 0.5938\n",
      "[1, 11600] loss : 1.1731, acc : 0.5312\n",
      "[1, 11700] loss : 1.0132, acc : 0.7188\n",
      "[1, 11800] loss : 0.9555, acc : 0.7812\n",
      "[1, 11900] loss : 1.2607, acc : 0.5938\n",
      "[1, 12000] loss : 1.0082, acc : 0.6250\n",
      "[1, 12100] loss : 1.0279, acc : 0.5938\n",
      "[1, 12200] loss : 0.9300, acc : 0.8125\n",
      "[1, 12300] loss : 1.0206, acc : 0.6875\n",
      "[1, 12400] loss : 0.9656, acc : 0.6250\n",
      "[1, 12500] loss : 1.1003, acc : 0.6250\n",
      "[1, 12600] loss : 1.1013, acc : 0.5938\n",
      "[1, 12700] loss : 1.1105, acc : 0.6250\n",
      "[1, 12800] loss : 1.1501, acc : 0.7188\n",
      "[1, 12900] loss : 0.9343, acc : 0.7500\n",
      "[1, 13000] loss : 0.8693, acc : 0.7812\n",
      "[1, 13100] loss : 1.3090, acc : 0.5000\n",
      "[1, 13200] loss : 0.7822, acc : 0.7500\n",
      "[1, 13300] loss : 0.9834, acc : 0.6875\n",
      "[1, 13400] loss : 1.0413, acc : 0.6250\n",
      "[1, 13500] loss : 1.0197, acc : 0.6875\n",
      "[1, 13600] loss : 0.8384, acc : 0.7500\n",
      "[1, 13700] loss : 1.0302, acc : 0.6250\n",
      "[1, 13800] loss : 1.0271, acc : 0.5625\n",
      "[1, 13900] loss : 0.9839, acc : 0.7188\n",
      "[1, 14000] loss : 0.9177, acc : 0.6562\n",
      "[1, 14100] loss : 1.0450, acc : 0.6562\n",
      "[1, 14200] loss : 0.6937, acc : 0.8125\n",
      "[1, 14300] loss : 0.8183, acc : 0.6250\n",
      "[1, 14400] loss : 0.8160, acc : 0.7812\n",
      "[1, 14500] loss : 0.8168, acc : 0.6562\n",
      "[1, 14600] loss : 1.0575, acc : 0.6562\n",
      "[1, 14700] loss : 0.7341, acc : 0.7812\n",
      "[1, 14800] loss : 0.8755, acc : 0.6875\n",
      "[1, 14900] loss : 0.9435, acc : 0.6875\n",
      "[1, 15000] loss : 0.7148, acc : 0.8125\n",
      "[1, 15100] loss : 1.0429, acc : 0.6250\n",
      "[1, 15200] loss : 0.8194, acc : 0.7188\n",
      "[1, 15300] loss : 0.8031, acc : 0.6875\n",
      "[1, 15400] loss : 0.8126, acc : 0.6875\n",
      "[1, 15500] loss : 0.7374, acc : 0.8125\n",
      "[1, 15600] loss : 0.8899, acc : 0.6875\n",
      "[1, 15700] loss : 0.8195, acc : 0.8438\n",
      "[1, 15800] loss : 1.0709, acc : 0.6562\n",
      "[1, 15900] loss : 0.9299, acc : 0.7812\n",
      "[1, 16000] loss : 0.6245, acc : 0.8125\n",
      "[1, 16100] loss : 1.0032, acc : 0.6250\n",
      "[1, 16200] loss : 0.7141, acc : 0.8125\n",
      "[1, 16300] loss : 0.7653, acc : 0.7500\n",
      "[1, 16400] loss : 0.7231, acc : 0.7812\n",
      "[1, 16500] loss : 0.6967, acc : 0.8125\n",
      "[1, 16600] loss : 0.7999, acc : 0.7812\n",
      "[1, 16700] loss : 0.8897, acc : 0.6875\n",
      "[1, 16800] loss : 0.9325, acc : 0.7188\n",
      "[1, 16900] loss : 0.9770, acc : 0.7500\n",
      "[1, 17000] loss : 0.9059, acc : 0.5938\n",
      "[1, 17100] loss : 0.9470, acc : 0.6875\n",
      "[1, 17200] loss : 0.9031, acc : 0.5938\n",
      "[1, 17300] loss : 0.7994, acc : 0.7500\n",
      "[1, 17400] loss : 0.9276, acc : 0.5938\n",
      "[1, 17500] loss : 1.0406, acc : 0.6250\n",
      "[1, 17600] loss : 0.9089, acc : 0.6562\n",
      "[1, 17700] loss : 0.9798, acc : 0.6562\n",
      "Val - loss : 1.0600, acc : 0.6475\n",
      "Weights saved in train_results/classifier/20200723-182433/model-17732.weights\n",
      "[2, 17800] loss : 1.0589, acc : 0.7500\n",
      "[2, 17900] loss : 0.8278, acc : 0.7500\n",
      "[2, 18000] loss : 1.1154, acc : 0.6562\n",
      "[2, 18100] loss : 0.8369, acc : 0.7500\n",
      "[2, 18200] loss : 0.7760, acc : 0.8125\n",
      "[2, 18300] loss : 0.9255, acc : 0.8125\n",
      "[2, 18400] loss : 1.0726, acc : 0.6875\n",
      "[2, 18500] loss : 0.9456, acc : 0.7188\n",
      "[2, 18600] loss : 1.0749, acc : 0.6562\n",
      "[2, 18700] loss : 0.9209, acc : 0.7188\n",
      "[2, 18800] loss : 0.9741, acc : 0.6250\n",
      "[2, 18900] loss : 0.9202, acc : 0.7500\n",
      "[2, 19000] loss : 1.0513, acc : 0.7188\n",
      "[2, 19100] loss : 1.0120, acc : 0.5938\n",
      "[2, 19200] loss : 0.8462, acc : 0.6562\n",
      "[2, 19300] loss : 1.0197, acc : 0.6250\n",
      "[2, 19400] loss : 0.9051, acc : 0.7188\n",
      "[2, 19500] loss : 0.8012, acc : 0.7812\n",
      "[2, 19600] loss : 0.8991, acc : 0.5938\n",
      "[2, 19700] loss : 0.7662, acc : 0.7500\n",
      "[2, 19800] loss : 0.8632, acc : 0.7188\n",
      "[2, 19900] loss : 1.0752, acc : 0.5625\n",
      "[2, 20000] loss : 0.9128, acc : 0.6250\n",
      "[2, 20100] loss : 0.8375, acc : 0.6875\n",
      "[2, 20200] loss : 0.7246, acc : 0.7812\n",
      "[2, 20300] loss : 0.8012, acc : 0.7500\n",
      "[2, 20400] loss : 0.8152, acc : 0.7188\n",
      "[2, 20500] loss : 0.7488, acc : 0.8125\n",
      "[2, 20600] loss : 0.9251, acc : 0.6875\n",
      "[2, 20700] loss : 0.8316, acc : 0.7188\n",
      "[2, 20800] loss : 0.7995, acc : 0.7500\n",
      "[2, 20900] loss : 0.9673, acc : 0.5938\n",
      "[2, 21000] loss : 0.6017, acc : 0.8125\n",
      "[2, 21100] loss : 1.0100, acc : 0.7812\n",
      "[2, 21200] loss : 0.9087, acc : 0.7188\n",
      "[2, 21300] loss : 0.6087, acc : 0.8438\n",
      "[2, 21400] loss : 0.7286, acc : 0.7500\n",
      "[2, 21500] loss : 0.7416, acc : 0.8125\n",
      "[2, 21600] loss : 1.0640, acc : 0.6875\n",
      "[2, 21700] loss : 0.8976, acc : 0.7188\n",
      "[2, 21800] loss : 0.7536, acc : 0.6562\n",
      "[2, 21900] loss : 0.5215, acc : 0.8438\n",
      "[2, 22000] loss : 1.0343, acc : 0.6250\n",
      "[2, 22100] loss : 0.8555, acc : 0.7188\n",
      "[2, 22200] loss : 0.7515, acc : 0.7812\n",
      "[2, 22300] loss : 0.7999, acc : 0.6875\n",
      "[2, 22400] loss : 0.7772, acc : 0.7500\n",
      "[2, 22500] loss : 0.7495, acc : 0.7812\n",
      "[2, 22600] loss : 0.4717, acc : 0.8438\n",
      "[2, 22700] loss : 1.1721, acc : 0.6250\n",
      "[2, 22800] loss : 0.6423, acc : 0.8438\n",
      "[2, 22900] loss : 1.0644, acc : 0.6250\n",
      "[2, 23000] loss : 0.8587, acc : 0.6875\n",
      "[2, 23100] loss : 0.8194, acc : 0.7812\n",
      "[2, 23200] loss : 0.8824, acc : 0.6562\n",
      "[2, 23300] loss : 0.7425, acc : 0.7500\n",
      "[2, 23400] loss : 1.0638, acc : 0.6562\n",
      "[2, 23500] loss : 0.5712, acc : 0.7812\n",
      "[2, 23600] loss : 0.7632, acc : 0.7188\n",
      "[2, 23700] loss : 0.6962, acc : 0.7812\n",
      "[2, 23800] loss : 0.7111, acc : 0.8438\n",
      "[2, 23900] loss : 0.8051, acc : 0.6250\n",
      "[2, 24000] loss : 1.0456, acc : 0.6562\n",
      "[2, 24100] loss : 0.5629, acc : 0.8125\n",
      "[2, 24200] loss : 0.9539, acc : 0.7188\n",
      "[2, 24300] loss : 0.8410, acc : 0.6562\n",
      "[2, 24400] loss : 0.8418, acc : 0.6875\n",
      "[2, 24500] loss : 0.6336, acc : 0.8750\n",
      "[2, 24600] loss : 1.1381, acc : 0.5938\n",
      "[2, 24700] loss : 0.6971, acc : 0.7812\n",
      "[2, 24800] loss : 0.6563, acc : 0.8438\n",
      "[2, 24900] loss : 0.6314, acc : 0.7500\n",
      "[2, 25000] loss : 0.7201, acc : 0.8125\n",
      "[2, 25100] loss : 0.6710, acc : 0.8750\n",
      "[2, 25200] loss : 0.8422, acc : 0.7500\n",
      "[2, 25300] loss : 0.6062, acc : 0.8125\n",
      "[2, 25400] loss : 0.5662, acc : 0.8438\n",
      "[2, 25500] loss : 0.7952, acc : 0.7188\n",
      "[2, 25600] loss : 0.8783, acc : 0.6875\n",
      "[2, 25700] loss : 0.7801, acc : 0.7812\n",
      "[2, 25800] loss : 1.0515, acc : 0.7188\n",
      "[2, 25900] loss : 0.6710, acc : 0.8125\n",
      "[2, 26000] loss : 0.7592, acc : 0.6875\n",
      "[2, 26100] loss : 0.5954, acc : 0.8750\n",
      "[2, 26200] loss : 0.5926, acc : 0.8750\n",
      "[2, 26300] loss : 0.6309, acc : 0.8125\n",
      "[2, 26400] loss : 0.8626, acc : 0.6562\n",
      "[2, 26500] loss : 0.7890, acc : 0.7188\n",
      "Val - loss : 0.9365, acc : 0.6797\n",
      "Weights saved in train_results/classifier/20200723-182433/model-26598.weights\n",
      "[3, 26600] loss : 0.7734, acc : 0.7500\n",
      "[3, 26700] loss : 0.7582, acc : 0.7188\n",
      "[3, 26800] loss : 0.6872, acc : 0.8438\n",
      "[3, 26900] loss : 0.7657, acc : 0.7188\n",
      "[3, 27000] loss : 0.8152, acc : 0.6875\n",
      "[3, 27100] loss : 0.7798, acc : 0.7188\n",
      "[3, 27200] loss : 0.6296, acc : 0.8438\n",
      "[3, 27300] loss : 0.6530, acc : 0.7812\n",
      "[3, 27400] loss : 0.6727, acc : 0.7812\n",
      "[3, 27500] loss : 0.7273, acc : 0.7812\n",
      "[3, 27600] loss : 0.6870, acc : 0.6875\n",
      "[3, 27700] loss : 0.8032, acc : 0.7188\n",
      "[3, 27800] loss : 0.6912, acc : 0.7500\n",
      "[3, 27900] loss : 0.5930, acc : 0.8750\n",
      "[3, 28000] loss : 0.7668, acc : 0.7812\n",
      "[3, 28100] loss : 0.6495, acc : 0.7812\n",
      "[3, 28200] loss : 0.7019, acc : 0.8125\n",
      "[3, 28300] loss : 1.0002, acc : 0.6562\n",
      "[3, 28400] loss : 0.6656, acc : 0.7812\n",
      "[3, 28500] loss : 0.4918, acc : 0.9062\n",
      "[3, 28600] loss : 0.6545, acc : 0.7188\n",
      "[3, 28700] loss : 0.9416, acc : 0.7188\n",
      "[3, 28800] loss : 0.7547, acc : 0.6875\n",
      "[3, 28900] loss : 0.6683, acc : 0.7188\n",
      "[3, 29000] loss : 0.6993, acc : 0.7812\n",
      "[3, 29100] loss : 0.5970, acc : 0.8438\n",
      "[3, 29200] loss : 0.7438, acc : 0.7188\n",
      "[3, 29300] loss : 0.6127, acc : 0.8125\n",
      "[3, 29400] loss : 0.5177, acc : 0.8750\n",
      "[3, 29500] loss : 0.6863, acc : 0.7812\n",
      "[3, 29600] loss : 0.7514, acc : 0.8125\n",
      "[3, 29700] loss : 0.5279, acc : 0.9062\n",
      "[3, 29800] loss : 0.7845, acc : 0.6875\n",
      "[3, 29900] loss : 0.6329, acc : 0.8125\n",
      "[3, 30000] loss : 0.6370, acc : 0.8125\n",
      "[3, 30100] loss : 0.4870, acc : 0.9062\n",
      "[3, 30200] loss : 0.9530, acc : 0.6250\n",
      "[3, 30300] loss : 0.6892, acc : 0.7500\n",
      "[3, 30400] loss : 0.4901, acc : 0.9062\n",
      "[3, 30500] loss : 0.6389, acc : 0.7500\n",
      "[3, 30600] loss : 0.7508, acc : 0.7812\n",
      "[3, 30700] loss : 0.7102, acc : 0.7500\n",
      "[3, 30800] loss : 0.6308, acc : 0.7500\n",
      "[3, 30900] loss : 0.8192, acc : 0.6562\n",
      "[3, 31000] loss : 0.5877, acc : 0.7500\n",
      "[3, 31100] loss : 0.5180, acc : 0.8438\n",
      "[3, 31200] loss : 0.6722, acc : 0.7500\n",
      "[3, 31300] loss : 0.7162, acc : 0.7812\n",
      "[3, 31400] loss : 0.8636, acc : 0.6250\n",
      "[3, 31500] loss : 0.7025, acc : 0.7812\n",
      "[3, 31600] loss : 0.6088, acc : 0.7500\n",
      "[3, 31700] loss : 0.7404, acc : 0.6875\n",
      "[3, 31800] loss : 0.6552, acc : 0.8125\n",
      "[3, 31900] loss : 0.8088, acc : 0.6562\n",
      "[3, 32000] loss : 0.4982, acc : 0.8750\n",
      "[3, 32100] loss : 0.7317, acc : 0.7500\n",
      "[3, 32200] loss : 0.5538, acc : 0.7812\n",
      "[3, 32300] loss : 0.5738, acc : 0.8750\n",
      "[3, 32400] loss : 0.4540, acc : 0.8750\n",
      "[3, 32500] loss : 0.6213, acc : 0.8125\n",
      "[3, 32600] loss : 0.6085, acc : 0.8125\n",
      "[3, 32700] loss : 0.3131, acc : 0.9062\n",
      "[3, 32800] loss : 0.4570, acc : 0.8750\n",
      "[3, 32900] loss : 0.4570, acc : 0.8438\n",
      "[3, 33000] loss : 0.5108, acc : 0.8750\n",
      "[3, 33100] loss : 0.6245, acc : 0.8125\n",
      "[3, 33200] loss : 0.4739, acc : 0.8438\n",
      "[3, 33300] loss : 0.3869, acc : 0.9062\n",
      "[3, 33400] loss : 0.4779, acc : 0.8750\n",
      "[3, 33500] loss : 0.6938, acc : 0.7500\n",
      "[3, 33600] loss : 0.5420, acc : 0.8438\n",
      "[3, 33700] loss : 0.7491, acc : 0.7812\n",
      "[3, 33800] loss : 0.6071, acc : 0.7188\n",
      "[3, 33900] loss : 0.5495, acc : 0.7500\n",
      "[3, 34000] loss : 0.5604, acc : 0.8125\n",
      "[3, 34100] loss : 0.5270, acc : 0.8438\n",
      "[3, 34200] loss : 0.5260, acc : 0.8750\n",
      "[3, 34300] loss : 0.4832, acc : 0.8125\n",
      "[3, 34400] loss : 0.6071, acc : 0.7812\n",
      "[3, 34500] loss : 0.5628, acc : 0.8125\n",
      "[3, 34600] loss : 0.9054, acc : 0.6875\n",
      "[3, 34700] loss : 0.6835, acc : 0.7188\n",
      "[3, 34800] loss : 0.5593, acc : 0.7500\n",
      "[3, 34900] loss : 0.6737, acc : 0.7188\n",
      "[3, 35000] loss : 0.7299, acc : 0.7188\n",
      "[3, 35100] loss : 0.3970, acc : 0.9062\n",
      "[3, 35200] loss : 0.5292, acc : 0.8438\n",
      "[3, 35300] loss : 0.4605, acc : 0.9062\n",
      "[3, 35400] loss : 0.7867, acc : 0.7500\n",
      "Val - loss : 0.9741, acc : 0.6826\n",
      "[4, 35500] loss : 0.6103, acc : 0.8125\n",
      "[4, 35600] loss : 0.5612, acc : 0.8750\n",
      "[4, 35700] loss : 0.5446, acc : 0.8125\n",
      "[4, 35800] loss : 0.5774, acc : 0.8438\n",
      "[4, 35900] loss : 0.5794, acc : 0.8438\n",
      "[4, 36000] loss : 0.5144, acc : 0.8125\n",
      "[4, 36100] loss : 0.5464, acc : 0.7500\n",
      "[4, 36200] loss : 0.4705, acc : 0.8750\n",
      "[4, 36300] loss : 0.6441, acc : 0.8438\n",
      "[4, 36400] loss : 0.7544, acc : 0.7500\n",
      "[4, 36500] loss : 0.5465, acc : 0.7812\n",
      "[4, 36600] loss : 0.5192, acc : 0.8438\n",
      "[4, 36700] loss : 0.4344, acc : 0.9062\n",
      "[4, 36800] loss : 0.2504, acc : 0.9375\n",
      "[4, 36900] loss : 0.6527, acc : 0.7188\n",
      "[4, 37000] loss : 0.3696, acc : 0.9688\n",
      "[4, 37100] loss : 0.5446, acc : 0.7812\n",
      "[4, 37200] loss : 0.5312, acc : 0.7188\n",
      "[4, 37300] loss : 0.5951, acc : 0.8125\n",
      "[4, 37400] loss : 0.3600, acc : 0.9062\n",
      "[4, 37500] loss : 0.4469, acc : 0.8750\n",
      "[4, 37600] loss : 0.7173, acc : 0.7188\n",
      "[4, 37700] loss : 0.6411, acc : 0.7188\n",
      "[4, 37800] loss : 0.5840, acc : 0.8438\n",
      "[4, 37900] loss : 0.6304, acc : 0.7812\n",
      "[4, 38000] loss : 0.6305, acc : 0.8438\n",
      "[4, 38100] loss : 0.5591, acc : 0.8750\n",
      "[4, 38200] loss : 0.6085, acc : 0.7812\n",
      "[4, 38300] loss : 0.5728, acc : 0.7812\n",
      "[4, 38400] loss : 0.5636, acc : 0.8438\n",
      "[4, 38500] loss : 0.5325, acc : 0.9062\n",
      "[4, 38600] loss : 0.6267, acc : 0.7812\n",
      "[4, 38700] loss : 0.6199, acc : 0.7812\n",
      "[4, 38800] loss : 0.5062, acc : 0.9062\n",
      "[4, 38900] loss : 0.6466, acc : 0.8125\n",
      "[4, 39000] loss : 0.5199, acc : 0.7812\n",
      "[4, 39100] loss : 0.8598, acc : 0.6875\n",
      "[4, 39200] loss : 0.9326, acc : 0.6250\n",
      "[4, 39300] loss : 0.7792, acc : 0.7500\n",
      "[4, 39400] loss : 0.6141, acc : 0.7812\n",
      "[4, 39500] loss : 0.7691, acc : 0.7500\n",
      "[4, 39600] loss : 0.4054, acc : 0.8438\n",
      "[4, 39700] loss : 0.3769, acc : 0.9062\n",
      "[4, 39800] loss : 0.6971, acc : 0.7812\n",
      "[4, 39900] loss : 0.3506, acc : 0.9062\n",
      "[4, 40000] loss : 0.4281, acc : 0.8750\n",
      "[4, 40100] loss : 0.5612, acc : 0.7812\n",
      "[4, 40200] loss : 0.6453, acc : 0.7188\n",
      "[4, 40300] loss : 0.4221, acc : 0.9062\n"
     ]
    }
   ],
   "source": [
    "#loss_fn = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32, device=device))\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "ckpt_weights_path = None\n",
    "best_loss = 1e10\n",
    "since_best = 0\n",
    "done = False\n",
    "\n",
    "if hparams['checkpoint']:\n",
    "    print_and_log('Resuming training from {}'.format(hparams['checkpoint']), log_file)\n",
    "    ckpt = torch.load(hparams['checkpoint'])\n",
    "    epoch = ckpt['epoch']\n",
    "    itr = ckpt['itr']\n",
    "    optimizer.load_state_dict(ckpt['optimizer'])\n",
    "    model.load_state_dict(ckpt['model'])\n",
    "    best_loss = ckpt['best_loss']\n",
    "else:\n",
    "    epoch = 0\n",
    "    itr = 0\n",
    "\n",
    "for epoch in range(epoch, hparams['n_epochs']):\n",
    "    if done:\n",
    "        break\n",
    "        \n",
    "    for batch in train_dataloader:\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "        itr += 1\n",
    "        features = batch[0].to(device)\n",
    "        labels = torch.tensor([inv_class_dict[i] for i in batch[1]['instrument_family_str']], \n",
    "                              dtype=torch.long, device=device)\n",
    "        outputs = model(features)\n",
    "        \n",
    "        loss = loss_fn(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (itr % hparams['display_iters'] == 0) or (itr == 1):\n",
    "            acc = (outputs.argmax(-1) == labels).float().mean()\n",
    "            print_and_log('[{}, {:5d}] loss : {:.4f}, acc : {:.4f}'.format(epoch, itr, loss.item(), acc.item()), log_file)\n",
    "            \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        ct, i, val_loss, val_acc = 0, 0, 0.0, 0.0\n",
    "        for batch in val_dataloader:\n",
    "            i += 1\n",
    "            ct += batch[0].size(0)\n",
    "            features = batch[0].to(device)\n",
    "            labels = torch.tensor([inv_class_dict[i] for i in batch[1]['instrument_family_str']], \n",
    "                                  dtype=torch.long, device=device)\n",
    "            outputs = model(features)\n",
    "\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            acc = (outputs.argmax(-1) == labels).float().mean()\n",
    "\n",
    "            val_loss += (loss.item() - val_loss)/i\n",
    "            val_acc += (acc.item() - val_acc)/i\n",
    "\n",
    "            if ct >= hparams['n_val_samples']:\n",
    "                break\n",
    "\n",
    "    print_and_log('Val - loss : {:.4f}, acc : {:.4f}'.format(val_loss, val_acc), log_file)\n",
    "\n",
    "    if val_loss < best_loss:\n",
    "        since_best = 0\n",
    "        best_loss = val_loss\n",
    "\n",
    "        # save weights\n",
    "        ckpt_weights_path = os.path.join(results_dir, 'model-{}.weights'.format(itr))\n",
    "        torch.save(model.state_dict(), ckpt_weights_path)\n",
    "        print_and_log('Weights saved in {}'.format(ckpt_weights_path), log_file)\n",
    "\n",
    "        # save meta information\n",
    "        ckpt_meta_path = os.path.join(results_dir, 'checkpoint')\n",
    "        torch.save({\n",
    "            'best_loss' : best_loss,\n",
    "            'epoch' : epoch,\n",
    "            'itr' : itr,\n",
    "            'optimizer' : optimizer.state_dict(),\n",
    "            'model' : model.state_dict()\n",
    "        }, ckpt_meta_path)\n",
    "    else:\n",
    "        since_best += 1\n",
    "        if since_best >= hparams['n_early_stopping']:\n",
    "            done = True\n",
    "            print_and_log('Early stopping... training complete', log_file)\n",
    "\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
